---
layout: post
excerpt: Donders Institute for Brain, Cognition and Behaviour - NeurIPS course
permalink: /Reinforce-and-Dreamer
published: true
images:
  - url: /assets/DreamerBanner.png
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

<h2><u>Content </u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In this work, we compare the world-model based learning agent Dreamerv2 with both evolutionary strategies and the Monte- Carlo policy gradient (REINFORCE) method. Dreamer is a model-based reinforcement model that learns a so-called ’world model’. The world model is a compact latent space representation of complex environment dynamics. This world model allows the agent to learn optimal policies within this latent space before applying them in the actual environment. The learning process can thus be separated into three distinct phases: the world model learning, the actor-critic learning, and of course applying the learned dynamics in the actual environment. The model is visualized in the image below:

<p><img src="/assets/DreamerBanner.png" /></p>

<p>All models are trained either on the Pong-v0 or Pinball Atari environments. We also provide the results of a random agent as a baseline. An example frame of the Pong-v0 game can be seen below.</p>

<p><img src="/assets/PongGame.png" /></p>

<p>We compare the different implementations in terms of model complexity, training time and final performance. We further investigate Dreamers sensitivity to changing the size of categorical latent space.</p>

<p><img src="/assets/dreamer-results-pong.png" /></p>

<p> Main reference: <a href="https://arxiv.org/abs/2010.02193">Mastering Atari with Discrete World Models</a></p>
